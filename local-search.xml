<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>tensorflow实现ResNet</title>
    <link href="/2024/11/13/tensorflow%E5%AE%9E%E7%8E%B0resnet/"/>
    <url>/2024/11/13/tensorflow%E5%AE%9E%E7%8E%B0resnet/</url>
    
    <content type="html"><![CDATA[<p>ResNet即深度残差网络，由何恺明及其团队提出，是深度学习领域又一具有开创性的工作，通过对残差结构的运用，ResNet使得训练数百层的网络成为了可能，从而具有非常强大的表征能力，其网络结构如图所示<br><img src="https://files.catbox.moe/lsomlz.jpg" alt="ResNet结构"></p><p>ResNet的核心是残差结构，如图所示<br><img src="https://files.catbox.moe/kudh2q.jpg" alt=""></p><p>在残差结构中，ResNet不再让下一层直接拟合我们想得到的底层映射，而是令其对一种残差映射进行拟合。若期望得到的底层映射为H(x)，我们令堆叠的非线性层拟合另一个映射F(x) := H(x) – x，则原有映射变为F(x) + x。对这种新的残差映射进行优化时，要比优化原有的非相关映射更为容易。不妨考虑极限情况，如果一个恒等映射是最优的，那么将残差向零逼近显然会比利用大量非线性层直接进行拟合更容易</p><p>ResNet引入残差结构最主要的目的是解决网络层数不断加深时导致的梯度消失问题，如果只是简单地堆叠更多层数，就会导致梯度消失（爆炸）问题，它从根源上导致了函数无法收敛。然而，通过标准初始化（normalized initialization）以及中间标准化层（intermediate normalization layer），已经可以较好地解决这个问题了，这使得深度为数十层的网络在反向传播过程中，可以通过随机梯度下降（SGD）的方式开始收敛。</p><p>但是，当深度更深的网络也可以开始收敛时，网络退化的问题就显露了出来：随着网络深度的增加，准确率先是达到瓶颈（这是很常见的），然后便开始迅速下降。需要注意的是，这种退化并不是由过拟合引起的。对于一个深度比较合适的网络来说，继续增加层数反而会导致训练错误率的提升，下图就是一个很好的例子<br><img src="https://files.catbox.moe/a24u2j.jpg" alt=""></p><p>ResNet解决的正是这个问题，其核心思路为：对一个准确率达到饱和的浅层网络，在它后面加几个恒等映射层（即y = x，输出等于输入），增加网络深度的同时不增加误差。这使得神经网络的层数可以超越之前的约束，提高准确率。下图展示了具体的残差结构<br><img src="https://files.catbox.moe/4290j8.jpg" alt=""></p><p>上图中的实线和虚线均表示恒等映射，实线表示通道相同，计算方式为H(x) = F(x) + x；虚线表示通道不同，计算方式为H(x) = F(x) + Wx，其中W为卷积操作，目的是调整x的维度（通道数）</p><p>可以借助tf.keras来实现这种残差结构，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ResnetBlock</span>(<span class="hljs-params"><span class="hljs-built_in">input</span>,filters,strides,residual_path</span>):<br>    residual=<span class="hljs-built_in">input</span><br>    x=tf.keras.layers.Conv2D(filters=filters,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>)(<span class="hljs-built_in">input</span>)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=filters,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>)(x)<br>    y=tf.keras.layers.BatchNormalization()(x)<br><br>    <span class="hljs-keyword">if</span> residual_path:<br>        residual=tf.keras.layers.Conv2D(filters=filters,kernel_size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>)(<span class="hljs-built_in">input</span>)<br>        residual=tf.keras.layers.BatchNormalization()(residual)<br><br>    output=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(y+residual)<br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>利用这个基本结构就可以搭建出ResNet模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Resnet18</span>(<span class="hljs-params">block_list,initial_filters</span>):<br>    block_num=<span class="hljs-built_in">len</span>(block_list)<br>    out_filters=initial_filters<br><br>    inputs=tf.keras.Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>))<br>    x=tf.keras.layers.Conv2D(filters=out_filters,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>,kernel_initializer=<span class="hljs-string">&#x27;he_normal&#x27;</span>)(inputs)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br><br>    <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(block_num):<br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(block_list[block_id]):<br>            <span class="hljs-keyword">if</span> block_id!=<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> layer_id==<span class="hljs-number">0</span>:<br>                x=ResnetBlock(x,filters=out_filters,strides=<span class="hljs-number">2</span>,residual_path=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">else</span>:<br>                x=ResnetBlock(x,filters=out_filters,strides=<span class="hljs-number">1</span>,residual_path=<span class="hljs-literal">False</span>)<br>        out_filters*=<span class="hljs-number">2</span><br><br>    x=tf.keras.layers.GlobalAveragePooling2D()(x)<br>    output=tf.keras.layers.Dense(<span class="hljs-number">10</span>,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>,kernel_regularizer=tf.keras.regularizers.l2())(x)<br>    model=tf.keras.Model(inputs,output)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>参数block_list表示ResNet中block的数量；initial_filters表示初始的卷积核数量。可以看到该模型同样使用了全局平均池化的方式来替代全连接层。</p><p>总体上看，ResNet取得的成果还是相当巨大的，它将网络深度提升到了152层，于2015年将ImageNet图像识别Top5错误率降至3.57 %</p><p>完整利用ResNet训练difar10数据集的代码：<a href="https://github.com/yuiasami/tensorflow_cnn/tree/main">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：LRDUNet</title>
    <link href="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/"/>
    <url>/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/</url>
    
    <content type="html"><![CDATA[<h1>基本信息</h1><p>论文题目：U-Net based neural network for fringe pattern denoising<br><a href="https://doi.org/10.1016/j.optlaseng.2021.106829"></a></p><p>本文主要创新点：提出了一种改进的用于去噪的UNet，运用了群卷积（grouped convolution）在减少网络训练参数的同时去噪效果明显好于其他网络</p><p><img src="https://files.catbox.moe/wrs11h.png" alt="title"></p><h1>网络架构</h1><p>本文提出的LRDUNet对比传统的UNet，最大的区别在于在同一层之间将2*2卷积与relu激活换成了本文提出的去噪块。其中，橙色箭头代表对输入特征图进行3*3卷积后再经过PReLU激活；天蓝色箭头代表对输入特征图进行1*1卷积后再经过PReLU激活，即改变输出的特征图数目</p><p><img src="https://files.catbox.moe/3j4oko.png" alt="fig1"></p><p><img src="https://files.catbox.moe/is2ron.png" alt="fig2"></p><p>网络核心的去噪块结构如下，首先对输入特征图进行1*1卷积以减少特征图数量，然后经过两次3*3群卷积，最后通过1*1卷积将前面所有的特征图整合到一起，生成与输入维度相同的输出特征图。</p><p><img src="https://files.catbox.moe/o6juwl.png" alt="fig34"></p><p>群卷积减少网络训练参数：假设卷积核大小为k*k，输入特征图数为f，输出特征图数（卷积核个数）为h，那么，一个卷积核需要训练的参数的数量为k*k*f，总的训练参数的数量为k*k*f*h+h（最后还要加个h是因为卷积完后还有一个偏置项）；但如果将输入特征图g等分，那么总的训练参数的数量为k*k*f/g*h+h</p><h1>实验结果</h1><p>首先对比了不同的网络结构的效果</p><p><img src="https://files.catbox.moe/5vm43t.png" alt="table1"></p><p>然后与其他网络比较了效果与参数量大小</p><p><img src="https://files.catbox.moe/8fczqg.png" alt="table2"></p><p><img src="https://files.catbox.moe/3jmfqy.png" alt="table3"></p><p>最后展示了实验结果</p><p><img src="https://files.catbox.moe/jytdza.png" alt="result1"></p><p><img src="https://files.catbox.moe/ehrkie.png" alt="result2"></p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow实现inceptionNet</title>
    <link href="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/"/>
    <url>/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/</url>
    
    <content type="html"><![CDATA[<p>InceptionNet即GoogLeNet，诞生于2015年，旨在通过增加网络的宽度来提升网络的能力，与VGGNet通过卷积层堆叠的方式（纵向）相比，是一个不同的方向（横向）</p><p>每一个inceptionNet都由基本单元构成，单元结构如下</p><img src="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/netunit.jpg" class="" title="网络基本单元"><p>因为每一个小的卷积单元包含卷积，批标准化和relu激活，所以可以先将其封装成一个函数，避免后续代码过于重复</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ConvBNRelu</span>(<span class="hljs-params">x,channel,kernel_size,strides,padding</span>):<br>    x=tf.keras.layers.Conv2D(channel,kernel_size,strides=strides,padding=padding)(x)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>基本单元代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inception_block</span>(<span class="hljs-params">x,channel,strides</span>):<br>    x1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x2_1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    x2_2=ConvBNRelu(x2_1,channel=channel,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x3_1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    x3_2=ConvBNRelu(x3_1,channel=channel,kernel_size=<span class="hljs-number">5</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x4_1=tf.keras.layers.MaxPool2D(<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)(x)<br>    x4_2=ConvBNRelu(x4_1,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x=tf.concat([x1,x2_2,x3_2,x4_2],axis=<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>有了基本单元后，就可以用它搭建所需的网络结构了，inceptionNet v1的结构图如下</p><img src="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/net.jpg" class="" title="网络基本结构"><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inception10</span>(<span class="hljs-params">blocks_num,classes_num,init_ch=<span class="hljs-number">16</span></span>):<br>    <span class="hljs-built_in">input</span>=tf.keras.Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>))<br>    x=ConvBNRelu(<span class="hljs-built_in">input</span>,init_ch,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    out_channels=init_ch<br>    <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(blocks_num):<br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>            <span class="hljs-keyword">if</span> layer_id==<span class="hljs-number">0</span>:<br>                x=inception_block(x,channel=out_channels,strides=<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">else</span>:<br>                x=inception_block(x,channel=out_channels,strides=<span class="hljs-number">1</span>)<br>        out_channels*=<span class="hljs-number">2</span><br><br>    x=tf.keras.layers.GlobalAveragePooling2D()(x)<br>    output=tf.keras.layers.Dense(classes_num,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(x)<br>    model=tf.keras.Model(<span class="hljs-built_in">input</span>,output)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>参数blocks_num代表InceptionNet的Block数，每个Block由两个基本单元构成，每经过一个Block，特征图尺寸变为1/2，通道数变为2倍；classes_num代表分类数；init_ch代表初始通道数，也即InceptionNet基本单元的初始卷积核个数</p><p>完整的利用cifar10数据集完成训练的代码：<a href="https://github.com/yuiasami/tensorflow_cnn">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>inceptionNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
