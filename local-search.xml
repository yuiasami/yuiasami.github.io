<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记：LRDUNet</title>
    <link href="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/"/>
    <url>/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/</url>
    
    <content type="html"><![CDATA[<h1>基本信息</h1><p>论文题目：U-Net based neural network for fringe pattern denoising<br><a href="https://doi.org/10.1016/j.optlaseng.2021.106829"></a></p><p>本文主要创新点：提出了一种改进的用于去噪的UNet，运用了群卷积（grouped convolution）在减少网络训练参数的同时去噪效果明显好于其他网络</p><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/title.png" class="" width="1"><h1>网络架构</h1><p>本文提出的LRDUNet对比传统的UNet，最大的区别在于在同一层之间将2*2卷积与relu激活换成了本文提出的去噪块。其中，橙色箭头代表对输入特征图进行3*3卷积后再经过PReLU激活；天蓝色箭头代表对输入特征图进行1*1卷积后再经过PReLU激活，即改变输出的特征图数目</p><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/fig1.png" class="" width="2"><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/fig2.png" class="" width="3"><p>网络核心的去噪块结构如下，首先对输入特征图进行1*1卷积以减少特征图数量，然后经过两次3*3群卷积，最后通过1*1卷积将前面所有的特征图整合到一起，生成与输入维度相同的输出特征图。</p><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/fig34.png" class="" width="4"><p>群卷积减少网络训练参数：假设卷积核大小为k*k，输入特征图数为f，输出特征图数（卷积核个数）为h，那么，一个卷积核需要训练的参数的数量为k*k*f，总的训练参数的数量为k*k*f*h+h（最后还要加个h是因为卷积完后还有一个偏置项）；但如果将输入特征图g等分，那么总的训练参数的数量为k*k*f/g*h+h</p><h1>实验结果</h1><p>首先对比了不同的网络结构的效果</p><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/table1.png" class="" width="5"><p>然后与其他网络比较了效果与参数量大小</p><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/table2.png" class="" width="6"><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/table3.png" class="" width="7"><p>最后展示了实验结果</p><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/result1.png" class="" width="8"><img src="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/result2.png" class="" width="9">]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow实现inceptionNet</title>
    <link href="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/"/>
    <url>/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/</url>
    
    <content type="html"><![CDATA[<p>InceptionNet即GoogLeNet，诞生于2015年，旨在通过增加网络的宽度来提升网络的能力，与VGGNet通过卷积层堆叠的方式（纵向）相比，是一个不同的方向（横向）</p><p>每一个inceptionNet都由基本单元构成，单元结构如下</p><img src="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/netunit.jpg" class="" title="网络基本单元"><p>因为每一个小的卷积单元包含卷积，批标准化和relu激活，所以可以先将其封装成一个函数，避免后续代码过于重复</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ConvBNRelu</span>(<span class="hljs-params">x,channel,kernel_size,strides,padding</span>):<br>    x=tf.keras.layers.Conv2D(channel,kernel_size,strides=strides,padding=padding)(x)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>基本单元代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inception_block</span>(<span class="hljs-params">x,channel,strides</span>):<br>    x1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x2_1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    x2_2=ConvBNRelu(x2_1,channel=channel,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x3_1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    x3_2=ConvBNRelu(x3_1,channel=channel,kernel_size=<span class="hljs-number">5</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x4_1=tf.keras.layers.MaxPool2D(<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)(x)<br>    x4_2=ConvBNRelu(x4_1,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x=tf.concat([x1,x2_2,x3_2,x4_2],axis=<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>有了基本单元后，就可以用它搭建所需的网络结构了，inceptionNet v1的结构图如下</p><img src="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/net.jpg" class="" title="网络基本结构"><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inception10</span>(<span class="hljs-params">blocks_num,classes_num,init_ch=<span class="hljs-number">16</span></span>):<br>    <span class="hljs-built_in">input</span>=tf.keras.Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>))<br>    x=ConvBNRelu(<span class="hljs-built_in">input</span>,init_ch,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    out_channels=init_ch<br>    <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(blocks_num):<br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>            <span class="hljs-keyword">if</span> layer_id==<span class="hljs-number">0</span>:<br>                x=inception_block(x,channel=out_channels,strides=<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">else</span>:<br>                x=inception_block(x,channel=out_channels,strides=<span class="hljs-number">1</span>)<br>        out_channels*=<span class="hljs-number">2</span><br><br>    x=tf.keras.layers.GlobalAveragePooling2D()(x)<br>    output=tf.keras.layers.Dense(classes_num,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(x)<br>    model=tf.keras.Model(<span class="hljs-built_in">input</span>,output)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>参数blocks_num代表InceptionNet的Block数，每个Block由两个基本单元构成，每经过一个Block，特征图尺寸变为1/2，通道数变为2倍；classes_num代表分类数；init_ch代表初始通道数，也即InceptionNet基本单元的初始卷积核个数</p><p>完整的利用cifar10数据集完成训练的代码：<a href="https://github.com/yuiasami/tensorflow_cnn">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>inceptionNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
