<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>tensorflow实现AlexNet</title>
    <link href="/2024/11/27/tensorflow%E5%AE%9E%E7%8E%B0AlexNet/"/>
    <url>/2024/11/27/tensorflow%E5%AE%9E%E7%8E%B0AlexNet/</url>
    
    <content type="html"><![CDATA[<p>AlexNet网络诞生于2012年，其ImageNet Top5错误率为16.4 %，可以说AlexNet的出现使得已经沉寂多年的深度学习领域开启了黄金时代。</p><p>AlexNet的总体结构和LeNet5有相似之处，但是有一些很重要的改进：<br>1.由五层卷积、三层全连接组成，输入图像尺寸为224 * 224 * 3，网络规模远大于LeNet5；<br>2.使用了Relu激活函数；<br>3.进行了Dropout操作，以防止模型过拟合，提升鲁棒性；<br>4.增加了一些训练上的技巧，包括数据增强、学习率衰减、权重衰减（L2正则化）等。<br>AlexNet的网络结构如图所示</p><p><img src="https://files.catbox.moe/tzdpmk.png" alt=""></p><p>在Tensorflow框架下利用Keras来搭建AlexNet模型，这里做了一些调整，将输入图像尺寸改为32* 32*3以适应cifar10数据集，并且将原始的AlexNet模型中的11*11、7*7、5*5等大尺寸卷积核均替换成了3*3的小卷积核,代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">AlexNet</span>():<br>    inputs=tf.keras.Input(shape=(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>))<br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">96</span>,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;valid&#x27;</span>)(inputs)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    x=tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">2</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">256</span>,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;valid&#x27;</span>)(x)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    x=tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">2</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">384</span>,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),padding=<span class="hljs-string">&#x27;same&#x27;</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">384</span>,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),padding=<span class="hljs-string">&#x27;same&#x27;</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">256</span>,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    x=tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">2</span>)(x)<br><br>    x=tf.keras.layers.Flatten()(x)<br>    x=tf.keras.layers.Dense(<span class="hljs-number">2048</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    x=tf.keras.layers.Dropout(<span class="hljs-number">0.5</span>)(x)<br><br>    x=tf.keras.layers.Dense(<span class="hljs-number">2048</span>,activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    x=tf.keras.layers.Dropout(<span class="hljs-number">0.5</span>)(x)<br><br>    output=tf.keras.layers.Dense(<span class="hljs-number">10</span>,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(x)<br>    model=tf.keras.Model(inputs,output)<br><br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>可以看到卷积操作共进行了5次：<br>第1次卷积：共有96个3*3的卷积核，不进行全零填充，进行BN操作，激活函数为Relu，进行最大池化，池化核尺寸为3*3，步长为2；<br>第2次卷积：与第1次卷积类似，除卷积核个数由96增加到256之外几乎相同；<br>第3次卷积：共有384个3*3的卷积核，进行全零填充，激活函数为Relu，不进行BN操作以及最大池化；<br>第4次卷积：与第3次卷积几乎完全相同；<br>第5次卷积：共有96个3*3的卷积核，进行全零填充，激活函数为Relu，不进行BN操作，进行最大池化，池化核尺寸为3*3，步长为2</p><p>全连接部分，共有三层：<br>第一层共2048个神经元，激活函数为Relu，进行0.5的dropout；<br>第二层与第一层几乎完全相同；<br>第三层共10个神经元，进行10分类。</p><p>可以看到，与结构类似的LeNet5相比，AlexNet模型的参数量有了非常明显的提升，卷积运算的层数也更多了，这有利于更好地提取特征；Relu激活函数的使用加快了模型的训练速度；Dropout的使用提升了模型的鲁棒性，这些优势使得AlexNet的性能大大提升</p><p>完整使用AlexNet对cifar10数据集进行分类的代码<a href="https://github.com/yuiasami/tensorflow_cnn">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow实现LeNet</title>
    <link href="/2024/11/27/tensorflow%E5%AE%9E%E7%8E%B0LeNet/"/>
    <url>/2024/11/27/tensorflow%E5%AE%9E%E7%8E%B0LeNet/</url>
    
    <content type="html"><![CDATA[<p>LeNet即LeNet5，由Yann LeCun在1998年提出，做为最早的卷积神经网络之一，是许多神经网络架构的起点，其网络结构如图所示</p><p><img src="https://files.catbox.moe/v099hd.jpg" alt=""></p><p>模型搭建具体流程如下：<br>1.输入图像大小为32 * 32 * 3，三通道彩色图像输入；<br>2.进行卷积，卷积核大小为5 * 5，个数为6，步长为1，不进行全零填充；<br>3.将卷积结果输入sigmoid激活函数（非线性函数）进行激活；<br>4.进行最大池化，池化核大小为2 * 2，步长为2；<br>5.进行卷积，卷积核大小为5 * 5，个数为16，步长为1，不进行全零填充；<br>6.将卷积结果输入sigmoid激活函数进行激活；<br>7.进行最大池化，池化核大小为2 * 2，步长为2；<br>8.输入三层全连接网络进行10分类。</p><p>具体实现代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LeNet</span>():<br>    inputs=tf.keras.Input(shape=(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>))<span class="hljs-comment">#Dense连接层必须指明输入维度</span><br><br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">6</span>,kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;valid&#x27;</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)(inputs)<br>    x=tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),strides=<span class="hljs-number">2</span>,padding=<span class="hljs-string">&#x27;valid&#x27;</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=<span class="hljs-number">16</span>,kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;valid&#x27;</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)(x)<br>    x=tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),strides=<span class="hljs-number">2</span>,padding=<span class="hljs-string">&#x27;valid&#x27;</span>)(x)<br><br>    x=tf.keras.layers.Flatten()(x)<br>    x=tf.keras.layers.Dense(<span class="hljs-number">120</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)(x)<br>    x=tf.keras.layers.Dense(<span class="hljs-number">84</span>,activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)(x)<br>    output=tf.keras.layers.Dense(<span class="hljs-number">10</span>,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(x)<br><br>    model=tf.keras.Model(inputs,output)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>与最初的LeNet5网络结构相比，这里做了一点微调，输入图像尺寸为32 * 32 * 3，以适应cifar10数据集。模型中采用的激活函数有sigmoid和softmax，池化层均采用最大池化，以保留边缘特征</p><p>总体上看，诞生于1998年的LeNet5与如今一些主流的CNN网络相比，其结构可以说是相当简单，不过它成功地利用“卷积提取特征→全连接分类”的经典思路解决了手写数字识别的问题，对神经网络研究的发展有着很重要的意义。</p><p>完整使用LeNet对cifar10数据集进行分类的代码<a href="https://github.com/yuiasami/tensorflow_cnn">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>使用vits训练自己的语音模型</title>
    <link href="/2024/11/14/%E4%BD%BF%E7%94%A8vits%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/14/%E4%BD%BF%E7%94%A8vits%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>本文基于项目<a href="https://github.com/Plachtaa/VITS-fast-fine-tuning">VITS-fast-fine-tuning</a>中的本地训练指南<a href="https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/LOCAL.md">LOCAL.md</a>，加入了自己在训练过程中碰到的一些问题和解决方法</p><p>1.确保本地安装了python 3.8版本（虚拟环境也可以），CMake &amp; C/C++编译器（实测只安装CMake会在第6步时报错无法编译，推荐直接安装Visual Studio，会直接安装好编译环境）以及ffmpeg</p><p>2.克隆项目仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/Plachtaa/VITS-fast-fine-tuning.git<br></code></pre></td></tr></table></figure><p>3.安装相应依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -r requirements.txt<br></code></pre></td></tr></table></figure><p>但是项目中的requirments中没有指定gradio的版本，会导致后续推理时无法打开webui；以及numpy版本过低会导致后续无法用whsiper进行文本转换，因此放一个能用的requirments在下面，可以直接覆盖项目里的requirments.txt</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Cython</span>==<span class="hljs-number">0</span>.<span class="hljs-number">29</span>.<span class="hljs-number">21</span><br><span class="hljs-attribute">librosa</span>==<span class="hljs-number">0</span>.<span class="hljs-number">9</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">matplotlib</span>==<span class="hljs-number">3</span>.<span class="hljs-number">3</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">scikit</span>-learn==<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">scipy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">numpy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">24</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">tensorboard</span><br><span class="hljs-attribute">torch</span>==<span class="hljs-number">1</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">torchvision</span><br><span class="hljs-attribute">torchaudio</span>==<span class="hljs-number">0</span>.<span class="hljs-number">13</span>.<span class="hljs-number">1</span>+cu116<br><span class="hljs-attribute">unidecode</span>==<span class="hljs-number">1</span>.<span class="hljs-number">3</span>.<span class="hljs-number">8</span><br><span class="hljs-attribute">pyopenjtalk</span>-prebuilt<br><span class="hljs-attribute">jamo</span>==<span class="hljs-number">0</span>.<span class="hljs-number">4</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">pypinyin</span>==<span class="hljs-number">0</span>.<span class="hljs-number">53</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">jieba</span>==<span class="hljs-number">0</span>.<span class="hljs-number">42</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">protobuf</span>==<span class="hljs-number">5</span>.<span class="hljs-number">28</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">cn2an</span>==<span class="hljs-number">0</span>.<span class="hljs-number">5</span>.<span class="hljs-number">22</span><br><span class="hljs-attribute">inflect</span>==<span class="hljs-number">7</span>.<span class="hljs-number">4</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">eng_to_ipa</span><br><span class="hljs-attribute">ko_pron</span><br><span class="hljs-attribute">indic_transliteration</span>==<span class="hljs-number">2</span>.<span class="hljs-number">3</span>.<span class="hljs-number">37</span><br><span class="hljs-attribute">num_thai</span>==<span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">5</span><br><span class="hljs-attribute">opencc</span>==<span class="hljs-number">1</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">demucs</span><br><span class="hljs-attribute">git</span>+https://github.com/openai/whisper.git <br><span class="hljs-attribute">gradio</span>==<span class="hljs-number">3</span>.<span class="hljs-number">47</span>.<span class="hljs-number">0</span><br><br><span class="hljs-attribute">moviepy</span>==<span class="hljs-number">1</span>.<span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">openai</span>-whisper==<span class="hljs-number">20240930</span><br><span class="hljs-attribute">regex</span>==<span class="hljs-number">2024</span>.<span class="hljs-number">11</span>.<span class="hljs-number">6</span><br><span class="hljs-attribute">tqdm</span>==<span class="hljs-number">4</span>.<span class="hljs-number">67</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>4.安装GPU版本的pytorch（要求CUDA版本为11.6或11.7，实际上我的CUDA版本为11.2，安装11.6版本的也没有问题）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># CUDA 11.6</span><br>pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116<br><span class="hljs-comment"># CUDA 11.7</span><br>pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117<br></code></pre></td></tr></table></figure><p>5.安装处理视频数据的库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install imageio==2.4.1<br>pip install moviepy<br></code></pre></td></tr></table></figure><p>6.编译monotonic align</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> monotonic_align<br><span class="hljs-built_in">mkdir</span> monotonic_align<br>python setup.py build_ext --inplace<br><span class="hljs-built_in">cd</span> ..<br></code></pre></td></tr></table></figure><p>7.<a href="https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/sampled_audio4ft_v2.zip">下载辅助数据（可选）</a>并解压到项目根目录，然后运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> pretrained_models<br><span class="hljs-built_in">mkdir</span> video_data<br><span class="hljs-built_in">mkdir</span> raw_audio<br><span class="hljs-built_in">mkdir</span> denoised_audio<br><span class="hljs-built_in">mkdir</span> custom_character_voice<br><span class="hljs-built_in">mkdir</span> segmented_character_voice<br></code></pre></td></tr></table></figure><p>8.下载预训练模型，可选项有</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">CJE:</span> 三语（中日英）<br><span class="hljs-symbol">CJ:</span> 双语（中日）<br><span class="hljs-symbol">C:</span> 只有中文<br></code></pre></td></tr></table></figure><p>对于Linux用户</p><p>下载CJE模型，运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/D_trilingual.pth -O ./pretrained_models/D_0.pth<br>wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/G_trilingual.pth -O ./pretrained_models/G_0.pth<br>wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/configs/uma_trilingual.json -O ./configs/finetune_speaker.json<br></code></pre></td></tr></table></figure><p>下载CJ模型，运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/D_0-p.pth -O ./pretrained_models/D_0.pth<br>wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/G_0-p.pth -O ./pretrained_models/G_0.pth<br>wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/config.json -O ./configs/finetune_speaker.json<br></code></pre></td></tr></table></figure><p>下载C模型，运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/D_0.pth -O ./pretrained_models/D_0.pth<br>wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/G_0.pth -O ./pretrained_models/G_0.pth<br>wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/config.json -O ./configs/finetune_speaker.json<br></code></pre></td></tr></table></figure><p>对于windows用户，手动从上方命令中的地址下载 <code>G_0.pth</code>, <code>D_0.pth</code>, <code>finetune_speaker.json</code></p><p>把所有<code>G</code>开头的模型重命名为 <code>G_0.pth</code>, <code>D</code> 开头的模型重命名为 <code>D_0.pth</code>, 把json文件重命名为 <code>finetune_speaker.json</code>.<br>把 <code>G_0.pth</code>, <code>D_0.pth</code> 放在 <code>pretrained_models</code> 目录下<br>把 <code>finetune_speaker.json</code> 放在 <code>configs</code> 目录下</p><p>9.处理数据，把所有视频文件按照<a href="https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA.MD">DATA.MD</a>中的方式命名后放在video_data目录下</p><p>10.处理所有音频数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">python scripts/video2audio.py<br>python scripts/denoise_audio.py<br>python scripts/long_audio_transcribe.py --languages <span class="hljs-string">&quot;&#123;PRETRAINED_MODEL&#125;&quot;</span> --whisper_size large<br>python scripts/short_audio_transcribe.py --languages <span class="hljs-string">&quot;&#123;PRETRAINED_MODEL&#125;&quot;</span> --whisper_size large<br>python scripts/resample.py<br></code></pre></td></tr></table></figure><p>将{PRETRAINED_MODEL}根据之前下载的预训练模型换成CJE、CJ、C，例如，如果下载的是CJE，那么命令应该为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python scripts/long_audio_transcribe.py --languages <span class="hljs-string">&quot;CJE&quot;</span> --whisper_size lar<br></code></pre></td></tr></table></figure><p>确保你的GPU显存大于12G，否则将whisper_size后的参数换成medium或者small（实测消耗12.6G显存）</p><p>11.处理所有文本数据。如果你选择使用辅助数据，运行 <code>python preprocess_v2.py --add_auxiliary_data True --languages &quot;&#123;PRETRAINED_MODEL&#125;&quot;</code>，否则运行 <code>python preprocess_v2.py --languages &quot;&#123;PRETRAINED_MODEL&#125;&quot;</code>，同样，将{PRETRAINED_MODEL}根据之前下载的预训练模型换成CJE、CJ、C</p><p>12.开始训练，运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs <span class="hljs-string">&quot;&#123;Maximum_epochs&#125;&quot;</span> --drop_speaker_embed True<br></code></pre></td></tr></table></figure><p>将{Maximum_epochs}换成你希望的训练轮数，推荐100epochs以上</p><p>如果希望断点续训，将命令改为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs <span class="hljs-string">&quot;&#123;Maximum_epochs&#125;&quot;</span> --drop_speaker_embed False --cont True<br></code></pre></td></tr></table></figure><p>确保在OUTPUT_MODEL目录里面有你之前的<code>G_latest.pth</code> 和 <code>D_latest.pth</code></p><p>13，训练完成后，运行以下命令就可以使用你的模型进行推理了（原来的命令缺少了config路径导致无法推理，加上即可）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python VC_inference.py --model_dir ./OUTPUT_MODEL/G_latest.pth --config_dir ./OUTPUT_MODEL/config.json --share True<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow实现ResNet</title>
    <link href="/2024/11/13/tensorflow%E5%AE%9E%E7%8E%B0resnet/"/>
    <url>/2024/11/13/tensorflow%E5%AE%9E%E7%8E%B0resnet/</url>
    
    <content type="html"><![CDATA[<p>ResNet即深度残差网络，由何恺明及其团队提出，是深度学习领域又一具有开创性的工作，通过对残差结构的运用，ResNet使得训练数百层的网络成为了可能，从而具有非常强大的表征能力，其网络结构如图所示<br><img src="https://files.catbox.moe/lsomlz.jpg" alt="ResNet结构"></p><p>ResNet的核心是残差结构，如图所示<br><img src="https://files.catbox.moe/kudh2q.jpg" alt=""></p><p>在残差结构中，ResNet不再让下一层直接拟合我们想得到的底层映射，而是令其对一种残差映射进行拟合。若期望得到的底层映射为H(x)，我们令堆叠的非线性层拟合另一个映射F(x) := H(x) – x，则原有映射变为F(x) + x。对这种新的残差映射进行优化时，要比优化原有的非相关映射更为容易。不妨考虑极限情况，如果一个恒等映射是最优的，那么将残差向零逼近显然会比利用大量非线性层直接进行拟合更容易</p><p>ResNet引入残差结构最主要的目的是解决网络层数不断加深时导致的梯度消失问题，如果只是简单地堆叠更多层数，就会导致梯度消失（爆炸）问题，它从根源上导致了函数无法收敛。然而，通过标准初始化（normalized initialization）以及中间标准化层（intermediate normalization layer），已经可以较好地解决这个问题了，这使得深度为数十层的网络在反向传播过程中，可以通过随机梯度下降（SGD）的方式开始收敛。</p><p>但是，当深度更深的网络也可以开始收敛时，网络退化的问题就显露了出来：随着网络深度的增加，准确率先是达到瓶颈（这是很常见的），然后便开始迅速下降。需要注意的是，这种退化并不是由过拟合引起的。对于一个深度比较合适的网络来说，继续增加层数反而会导致训练错误率的提升，下图就是一个很好的例子<br><img src="https://files.catbox.moe/a24u2j.jpg" alt=""></p><p>ResNet解决的正是这个问题，其核心思路为：对一个准确率达到饱和的浅层网络，在它后面加几个恒等映射层（即y = x，输出等于输入），增加网络深度的同时不增加误差。这使得神经网络的层数可以超越之前的约束，提高准确率。下图展示了具体的残差结构<br><img src="https://files.catbox.moe/4290j8.jpg" alt=""></p><p>上图中的实线和虚线均表示恒等映射，实线表示通道相同，计算方式为H(x) = F(x) + x；虚线表示通道不同，计算方式为H(x) = F(x) + Wx，其中W为卷积操作，目的是调整x的维度（通道数）</p><p>可以借助tf.keras来实现这种残差结构，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ResnetBlock</span>(<span class="hljs-params"><span class="hljs-built_in">input</span>,filters,strides,residual_path</span>):<br>    residual=<span class="hljs-built_in">input</span><br>    x=tf.keras.layers.Conv2D(filters=filters,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>)(<span class="hljs-built_in">input</span>)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br><br>    x=tf.keras.layers.Conv2D(filters=filters,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>)(x)<br>    y=tf.keras.layers.BatchNormalization()(x)<br><br>    <span class="hljs-keyword">if</span> residual_path:<br>        residual=tf.keras.layers.Conv2D(filters=filters,kernel_size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>)(<span class="hljs-built_in">input</span>)<br>        residual=tf.keras.layers.BatchNormalization()(residual)<br><br>    output=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(y+residual)<br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>利用这个基本结构就可以搭建出ResNet模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Resnet18</span>(<span class="hljs-params">block_list,initial_filters</span>):<br>    block_num=<span class="hljs-built_in">len</span>(block_list)<br>    out_filters=initial_filters<br><br>    inputs=tf.keras.Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>))<br>    x=tf.keras.layers.Conv2D(filters=out_filters,kernel_size=(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>,use_bias=<span class="hljs-literal">False</span>,kernel_initializer=<span class="hljs-string">&#x27;he_normal&#x27;</span>)(inputs)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br><br>    <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(block_num):<br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(block_list[block_id]):<br>            <span class="hljs-keyword">if</span> block_id!=<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> layer_id==<span class="hljs-number">0</span>:<br>                x=ResnetBlock(x,filters=out_filters,strides=<span class="hljs-number">2</span>,residual_path=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">else</span>:<br>                x=ResnetBlock(x,filters=out_filters,strides=<span class="hljs-number">1</span>,residual_path=<span class="hljs-literal">False</span>)<br>        out_filters*=<span class="hljs-number">2</span><br><br>    x=tf.keras.layers.GlobalAveragePooling2D()(x)<br>    output=tf.keras.layers.Dense(<span class="hljs-number">10</span>,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>,kernel_regularizer=tf.keras.regularizers.l2())(x)<br>    model=tf.keras.Model(inputs,output)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>参数block_list表示ResNet中block的数量；initial_filters表示初始的卷积核数量。可以看到该模型同样使用了全局平均池化的方式来替代全连接层。</p><p>总体上看，ResNet取得的成果还是相当巨大的，它将网络深度提升到了152层，于2015年将ImageNet图像识别Top5错误率降至3.57 %</p><p>完整利用ResNet训练difar10数据集的代码：<a href="https://github.com/yuiasami/tensorflow_cnn/tree/main">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记：LRDUNet</title>
    <link href="/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/"/>
    <url>/2024/11/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ALRDUNet/</url>
    
    <content type="html"><![CDATA[<h1>基本信息</h1><p>论文题目：U-Net based neural network for fringe pattern denoising<br><a href="https://doi.org/10.1016/j.optlaseng.2021.106829"></a></p><p>本文主要创新点：提出了一种改进的用于去噪的UNet，运用了群卷积（grouped convolution）在减少网络训练参数的同时去噪效果明显好于其他网络</p><p><img src="https://files.catbox.moe/wrs11h.png" alt="title"></p><h1>网络架构</h1><p>本文提出的LRDUNet对比传统的UNet，最大的区别在于在同一层之间将2*2卷积与relu激活换成了本文提出的去噪块。其中，橙色箭头代表对输入特征图进行3*3卷积后再经过PReLU激活；天蓝色箭头代表对输入特征图进行1*1卷积后再经过PReLU激活，即改变输出的特征图数目</p><p><img src="https://files.catbox.moe/3j4oko.png" alt="fig1"></p><p><img src="https://files.catbox.moe/is2ron.png" alt="fig2"></p><p>网络核心的去噪块结构如下，首先对输入特征图进行1*1卷积以减少特征图数量，然后经过两次3*3群卷积，最后通过1*1卷积将前面所有的特征图整合到一起，生成与输入维度相同的输出特征图。</p><p><img src="https://files.catbox.moe/o6juwl.png" alt="fig34"></p><p>群卷积减少网络训练参数：假设卷积核大小为k*k，输入特征图数为f，输出特征图数（卷积核个数）为h，那么，一个卷积核需要训练的参数的数量为k*k*f，总的训练参数的数量为k*k*f*h+h（最后还要加个h是因为卷积完后还有一个偏置项）；但如果将输入特征图g等分，那么总的训练参数的数量为k*k*f/g*h+h</p><h1>实验结果</h1><p>首先对比了不同的网络结构的效果</p><p><img src="https://files.catbox.moe/5vm43t.png" alt="table1"></p><p>然后与其他网络比较了效果与参数量大小</p><p><img src="https://files.catbox.moe/8fczqg.png" alt="table2"></p><p><img src="https://files.catbox.moe/3jmfqy.png" alt="table3"></p><p>最后展示了实验结果</p><p><img src="https://files.catbox.moe/jytdza.png" alt="result1"></p><p><img src="https://files.catbox.moe/ehrkie.png" alt="result2"></p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow实现inceptionNet</title>
    <link href="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/"/>
    <url>/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/</url>
    
    <content type="html"><![CDATA[<p>InceptionNet即GoogLeNet，诞生于2015年，旨在通过增加网络的宽度来提升网络的能力，与VGGNet通过卷积层堆叠的方式（纵向）相比，是一个不同的方向（横向）</p><p>每一个inceptionNet都由基本单元构成，单元结构如下</p><img src="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/netunit.jpg" class="" title="网络基本单元"><p>因为每一个小的卷积单元包含卷积，批标准化和relu激活，所以可以先将其封装成一个函数，避免后续代码过于重复</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ConvBNRelu</span>(<span class="hljs-params">x,channel,kernel_size,strides,padding</span>):<br>    x=tf.keras.layers.Conv2D(channel,kernel_size,strides=strides,padding=padding)(x)<br>    x=tf.keras.layers.BatchNormalization()(x)<br>    x=tf.keras.layers.Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>基本单元代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inception_block</span>(<span class="hljs-params">x,channel,strides</span>):<br>    x1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x2_1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    x2_2=ConvBNRelu(x2_1,channel=channel,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x3_1=ConvBNRelu(x,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    x3_2=ConvBNRelu(x3_1,channel=channel,kernel_size=<span class="hljs-number">5</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x4_1=tf.keras.layers.MaxPool2D(<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)(x)<br>    x4_2=ConvBNRelu(x4_1,channel=channel,kernel_size=<span class="hljs-number">1</span>,strides=strides,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    x=tf.concat([x1,x2_2,x3_2,x4_2],axis=<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>有了基本单元后，就可以用它搭建所需的网络结构了，inceptionNet v1的结构图如下</p><img src="/2024/11/06/tensorflow%E5%AE%9E%E7%8E%B0inceptionnet/net.jpg" class="" title="网络基本结构"><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inception10</span>(<span class="hljs-params">blocks_num,classes_num,init_ch=<span class="hljs-number">16</span></span>):<br>    <span class="hljs-built_in">input</span>=tf.keras.Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>))<br>    x=ConvBNRelu(<span class="hljs-built_in">input</span>,init_ch,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br><br>    out_channels=init_ch<br>    <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(blocks_num):<br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>            <span class="hljs-keyword">if</span> layer_id==<span class="hljs-number">0</span>:<br>                x=inception_block(x,channel=out_channels,strides=<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">else</span>:<br>                x=inception_block(x,channel=out_channels,strides=<span class="hljs-number">1</span>)<br>        out_channels*=<span class="hljs-number">2</span><br><br>    x=tf.keras.layers.GlobalAveragePooling2D()(x)<br>    output=tf.keras.layers.Dense(classes_num,activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(x)<br>    model=tf.keras.Model(<span class="hljs-built_in">input</span>,output)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>参数blocks_num代表InceptionNet的Block数，每个Block由两个基本单元构成，每经过一个Block，特征图尺寸变为1/2，通道数变为2倍；classes_num代表分类数；init_ch代表初始通道数，也即InceptionNet基本单元的初始卷积核个数</p><p>完整的利用cifar10数据集完成训练的代码：<a href="https://github.com/yuiasami/tensorflow_cnn">GitHub - yuiasami/tensorflow_cnn</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>inceptionNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
